{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKewMjlNe8bcDUdtTUoNPb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# init"
      ],
      "metadata": {
        "id": "lg1s0uCJXEMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")  # Fetches the token from Colab Secrets\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "elIrlcSpRNuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_error()  # Suppress progress bars and unnecessary logs"
      ],
      "metadata": {
        "id": "N10NQiEDTFGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines with hugging face"
      ],
      "metadata": {
        "id": "yR1WgzkXwHgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDU7qg3mvHt_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\") #model used for summarization\n",
        "\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming various industries by enabling machines to perform tasks\n",
        "that typically require human intelligence. These tasks include natural language processing, computer\n",
        "vision, speech recognition, and decision-making. AI models, particularly deep learning models, are\n",
        "trained on large datasets to recognize patterns and make predictions. In recent years, advancements in\n",
        "AI have led to applications such as self-driving cars, medical diagnosis, and chatbots. Despite its\n",
        "benefits, AI also raises ethical concerns, including bias in decision-making and the impact on employment.\n",
        "As AI continues to evolve, researchers and policymakers must address these challenges to ensure\n",
        "responsible AI development.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(text,\n",
        "                     max_length=50 #limit response to 50 tokens\n",
        "                     )\n",
        "\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-jMMx8NEZgO",
        "outputId": "32bb5878-436f-4dbe-adb0-61775005b2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI models, particularly deep learning models, are trained on large datasets to recognize patterns and make predictions. In recent years, advancements in AI have led to applications such as self-driving cars, medical diagnosis, and chatbots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"gpt2\")  # Model used for text generation\n",
        "\n",
        "# Example customer complaint\n",
        "customer_complaint = \"The product was horrible, and the service was very slow.\"\n",
        "\n",
        "# Predefined apology response\n",
        "apology_response = \"Dear client, we sincerely apologize for the inconvenience you reported\"\n",
        "\n",
        "# Create the prompt with structured input, this is how the response starts\n",
        "prompt = f\"\"\"Customer complaint: {customer_complaint}.\\nCompany's response to the customer: {apology_response}.  \"\"\"\n",
        "\n",
        "# Generate a response using the model\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_length=150,  # Limit response to 150 tokens\n",
        "    pad_token_id=generator.tokenizer.eos_token_id,  # Use EOS token to avoid padding issues (GPT-2 lacks a default pad token)\n",
        "    truncation=True  # Truncate input if it's too long\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(outputs[0][\"generated_text\"])  # Extract and display the generated response"
      ],
      "metadata": {
        "id": "06Zn3gcXRaK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34dabd9-0c4b-4bce-de5c-bfefe1cfa2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer complaint: The product was horrible, and the service was very slow..\n",
            "Company's response to the customer: Dear client, we sincerely apologize for the inconvenience you reported.    A large number of people have reported some type of lag in their online experience with our online services. We understand that your situation might arise due to the nature of your account, but we would like to know about the way you handled this issue. The complaint was brought to our attention by our Customer Support team in a variety of ways, including by using in-person calls or sending out text messages. Please refer to our \"What are the reasons for your complaint?\" section for details. We take very seriously the experience of our customers, and strive to take action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"La inteligencia artificial esta cambiando el planeta\"\n",
        "\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "translations = translator(spanish_text,\n",
        "                          clean_up_tokenization_spaces=True #Fixes spacing issues\n",
        "                          )\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2qHD6NTTRX",
        "outputId": "62c4ed01-c4d4-49d6-db01-615701a55138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence is changing the planet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformers arch"
      ],
      "metadata": {
        "id": "Ifq_rlU5agKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç **What‚Äôs Different in Traditional NLP terms vs LLMs?**\n",
        "\n",
        "1Ô∏è‚É£ Encoding in Traditional NLP = Simple static embeddings (Word2Vec, TF-IDF).\n",
        "\n",
        "1Ô∏è‚É£.1Ô∏è‚É£ Encoding in LLMs = Context-aware representations (transformers track relationships between words).\n",
        "\n",
        "2Ô∏è‚É£ Decoding in Traditional NLP = Retrieving stored text or using basic grammar rules.\n",
        "\n",
        "2Ô∏è‚É£.1Ô∏è‚É£Decoding in LLMs = Generating new text using self-attention and autoregressive prediction."
      ],
      "metadata": {
        "id": "KosKu6B3fqdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| **Model Type**      | **Description**                                             | **Use Cases**                                      | **Example Models**               |\n",
        "|---------------------|------------------------------------------------|-------------------------------------------------|--------------------------------|\n",
        "| **Encoder**      | Reads and **understands** text, but doesn't generate new text | Classification, Sentiment Analysis, Named Entity Recognition (NER) | **BERT, RoBERTa, DistilBERT, ALBERT** |\n",
        "| **Decoder**    | **Generates** text based on input (auto-regressive)  | Text Completion, Chatbots, Story Writing | **GPT-2, GPT-3, LLaMA, Falcon** |\n",
        "| **Encoder-Decoder**  | First understands the input (encoder), then generates **transformed output** (decoder) | Summarization, Translation, Question Answering | **T5, BART, mBART, FLAN-T5** |\n"
      ],
      "metadata": {
        "id": "1DQA0He8dbnV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRCIhm97abn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DT9tsooeWrTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}