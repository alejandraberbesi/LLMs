{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN00J0qpgerh1euNQuO8hs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alejandraberbesi/LLMs/blob/main/Intro_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# init"
      ],
      "metadata": {
        "id": "lg1s0uCJXEMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")  # Fetches the token from Colab Secrets\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "elIrlcSpRNuR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_error()  # Suppress progress bars and unnecessary logs"
      ],
      "metadata": {
        "id": "N10NQiEDTFGH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines with hugging face"
      ],
      "metadata": {
        "id": "yR1WgzkXwHgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yDU7qg3mvHt_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\") #model used for summarization\n",
        "\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming various industries by enabling machines to perform tasks\n",
        "that typically require human intelligence. These tasks include natural language processing, computer\n",
        "vision, speech recognition, and decision-making. AI models, particularly deep learning models, are\n",
        "trained on large datasets to recognize patterns and make predictions. In recent years, advancements in\n",
        "AI have led to applications such as self-driving cars, medical diagnosis, and chatbots. Despite its\n",
        "benefits, AI also raises ethical concerns, including bias in decision-making and the impact on employment.\n",
        "As AI continues to evolve, researchers and policymakers must address these challenges to ensure\n",
        "responsible AI development.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(text,\n",
        "                     max_length=50 #limit response to 50 tokens\n",
        "                     )\n",
        "\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-jMMx8NEZgO",
        "outputId": "9e510460-865f-4819-869a-148e5456e128"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI models, particularly deep learning models, are trained on large datasets to recognize patterns and make predictions. In recent years, advancements in AI have led to applications such as self-driving cars, medical diagnosis, and chatbots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"gpt2\")  # Model used for text generation\n",
        "\n",
        "# Example customer complaint\n",
        "customer_complaint = \"The product was horrible, and the service was very slow.\"\n",
        "\n",
        "# Predefined apology response\n",
        "apology_response = \"Dear client, we sincerely apologize for the inconvenience you reported\"\n",
        "\n",
        "# Create the prompt with structured input, this is how the response starts\n",
        "prompt = f\"\"\"Customer complaint: {customer_complaint}.\\nCompany's response to the customer: {apology_response}.  \"\"\"\n",
        "\n",
        "# Generate a response using the model\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_length=150,  # Limit response to 150 tokens\n",
        "    pad_token_id=generator.tokenizer.eos_token_id,  # Use EOS token to avoid padding issues (GPT-2 lacks a default pad token)\n",
        "    truncation=True  # Truncate input if it's too long\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(outputs[0][\"generated_text\"])  # Extract and display the generated response"
      ],
      "metadata": {
        "id": "06Zn3gcXRaK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f3ad86-a6a7-459b-e313-a4bbbe9b4d6e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer complaint: The product was horrible, and the service was very slow..\n",
            "Company's response to the customer: Dear client, we sincerely apologize for the inconvenience you reported.  ***************************\n",
            "To be honest, the customer contacted me about four times. They have never contacted us about this and I really don't know what could have caused the customers problems. ***************************\n",
            "To us customers this could not be your fault, the complaint. It appears that these three products were purchased at the same prices as two other packages, which resulted in total delivery costs for consumers. While it would be hard to find good prices, the price tag is not outrageous at all. In fact, I cannot see any way for us to claim,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"La inteligencia artificial esta cambiando el planeta\"\n",
        "\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "translations = translator(spanish_text,\n",
        "                          clean_up_tokenization_spaces=True #Fixes spacing issues\n",
        "                          )\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2qHD6NTTRX",
        "outputId": "e0cb3571-d3c4-4179-9616-d8b4be1cb742"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence is changing the planet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformers architecture"
      ],
      "metadata": {
        "id": "Ifq_rlU5agKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç **What‚Äôs Different in Traditional NLP terms vs LLMs?**\n",
        "\n",
        "1Ô∏è‚É£ Encoding in Traditional NLP = Simple static embeddings (Word2Vec, TF-IDF).\n",
        "\n",
        "1Ô∏è‚É£.1Ô∏è‚É£ Encoding in LLMs = Context-aware representations (transformers track relationships between words).\n",
        "\n",
        "2Ô∏è‚É£ Decoding in Traditional NLP = Retrieving stored text or using basic grammar rules.\n",
        "\n",
        "2Ô∏è‚É£.1Ô∏è‚É£Decoding in LLMs = Generating new text using self-attention and autoregressive prediction."
      ],
      "metadata": {
        "id": "KosKu6B3fqdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| **Model Type**      | **Description**                                             | **Use Cases**                                      | **Example Models**               |\n",
        "|---------------------|------------------------------------------------|-------------------------------------------------|--------------------------------|\n",
        "| **Encoder**      | Reads and **understands** text, but doesn't generate new text | Classification, Sentiment Analysis, Named Entity Recognition (NER) | **BERT, RoBERTa, DistilBERT, ALBERT** |\n",
        "| **Decoder**    | **Generates** text based on input (auto-regressive)  | Text Completion, Chatbots, Story Writing | **GPT-2, GPT-3, LLaMA, Falcon** |\n",
        "| **Encoder-Decoder**  | First understands the input (encoder), then generates **transformed output** (decoder) | Summarization, Translation, Question Answering | **T5, BART, mBART, FLAN-T5** |\n"
      ],
      "metadata": {
        "id": "1DQA0He8dbnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_llm_architecture(model_pipeline):\n",
        "\n",
        "    # Get the model architecture\n",
        "    model = model_pipeline.model\n",
        "    model_type = model.config.architectures[0].lower()  # Get architecture type, and make lowercase\n",
        "\n",
        "    # Identify architecture based on the model type\n",
        "    if \"bert\" in model_type or \"roberta\" in model_type or \"deberta\" in model_type:\n",
        "        print(f\"the model arch of {model_type} is Encoder\")\n",
        "    elif \"gpt\" in model_type or \"opt\" in model_type or \"bloom\" in model_type:\n",
        "        print(f\"the model arch of {model_type} is Decoder\")\n",
        "    elif \"t5\" in model_type or \"bart\" in model_type or \"marian\" in model_type:\n",
        "        print(f\"the model arch of {model_type} is Encoder-Decoder\")\n",
        "    else:\n",
        "        print(f\"Other architecture type: {model_type}\")"
      ],
      "metadata": {
        "id": "BRCIhm97abn3"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.model.config.architectures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryh6_LsLyvks",
        "outputId": "b96d733a-a89f-462b-e02a-28c5da7480c6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T5ForConditionalGeneration']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identify_llm_architecture(summarizer)"
      ],
      "metadata": {
        "id": "DT9tsooeWrTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048b68b8-0ce1-4282-9488-396d4dc691dc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model arch of t5forconditionalgeneration is Encoder-Decoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identify_llm_architecture(generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVVoUZr5kZlM",
        "outputId": "920cb8d5-e96e-474d-d5ff-bad9f54e2b81"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model arch of gpt2lmheadmodel is Decoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "identify_llm_architecture(translator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGPTx7uikaMr",
        "outputId": "6b874bd0-3de0-4a1a-ed6c-1228d7debc44"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model arch of marianmtmodel is Encoder-Decoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  extractive question-answering.\n",
        "text= \"\"\"\n",
        "Bogot√° is the capital of and largest city in Colombia. It is a place of convergence for people from all around the country and is therefore diverse and multicultural. Within this city, the past and present come together.\n",
        "The city is also very green thanks to its parks and the hills that extend along its eastern boundary, dwarfed by their two highest points, Monserrate and Guadalupe. The landscape that the people of Bogot√° enjoy daily‚Äîthe sea of green that makes up the Andes mountain range, rising up in the east‚Äîwould be nearly impossible to find in any other large city.\n",
        "But Bogot√° also has the colors of modern buildings and authentic colonial roof tiles that are a true historical treasure from the colonial era.\n",
        "Thanks to this fusion of the past and present, Bogot√° is an ideal location with history, entertainment, delicious cuisine, culture, business, and much more.\n",
        "\"\"\"\n",
        "\n",
        "question=\"what are the characteristics of bogota?\"\n",
        "\n",
        "qa = pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "output = qa(question=question, context=text)\n",
        "\n",
        "print(output['answer'])\n",
        "print(\"----------------\")\n",
        "identify_llm_architecture(qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmRKEnpuko4",
        "outputId": "46673f11-f89b-4db1-8b84-69a3cb96a197"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history, entertainment, delicious cuisine, culture, business, and much more\n",
            "----------------\n",
            "the model arch of distilbertforquestionanswering is Encoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  generative question-answering\n",
        "\n",
        "qa2 = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa2(input_text,\n",
        "             max_new_tokens=50 # controls only the number of tokens generated by the model, not the input length\n",
        "             )\n",
        "\n",
        "print(output[0]['generated_text'])\n",
        "print(\"----------------\")\n",
        "identify_llm_architecture(qa2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haGEEVWlwBk9",
        "outputId": "3b190f24-3878-4d66-ef68-2548d5e9f6cd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: \n",
            "Bogot√° is the capital of and largest city in Colombia. It is a place of convergence for people from all around the country and is therefore diverse and multicultural. Within this city, the past and present come together.\n",
            "The city is also very green thanks to its parks and the hills that extend along its eastern boundary, dwarfed by their two highest points, Monserrate and Guadalupe. The landscape that the people of Bogot√° enjoy daily‚Äîthe sea of green that makes up the Andes mountain range, rising up in the east‚Äîwould be nearly impossible to find in any other large city.\n",
            "But Bogot√° also has the colors of modern buildings and authentic colonial roof tiles that are a true historical treasure from the colonial era.\n",
            "Thanks to this fusion of the past and present, Bogot√° is an ideal location with history, entertainment, delicious cuisine, culture, business, and much more.\n",
            "\n",
            "\n",
            "Question: what are the characteristics of bogota?\n",
            "\n",
            "Answer: the city contains a huge cultural tradition but at the same time a rich heritage. It's a place of art history and the best of the arts people. It's as natural as the water, of the flowers, or as warm as the trees,\n",
            "----------------\n",
            "the model arch of gpt2lmheadmodel is Decoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n-_gfqOztL2"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}